source('~/NUS/TU Delft/WI4231 Mathematical Data Science/Exercises/Week 3/TweetCleaner.R', echo=TRUE)
setwd("~/NUS/TU Delft/WI4231 Mathematical Data Science/Exercises/Week 3")
source('~/NUS/TU Delft/WI4231 Mathematical Data Science/Exercises/Week 3/TweetCleaner.R', echo=TRUE)
unlist(lapply(ngrams(words(x), 2), paste, collapse = " "), use.names = FALSE)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
v3
df <- tweets
df
source('~/.active-rstudio-document', echo=TRUE)
v3
source('~/.active-rstudio-document', echo=TRUE)
library(stringr)
source('~/.active-rstudio-document', echo=TRUE)
v3
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
m = as.matrix(tdm)
wordFreq <- sort(rowSums(m), decreasing=TRUE)
pal <- brewer.pal(9, "GnBu")
pal <- pal[-(1:4)]
set.seed(375) # to make it reproducible
grayLevels <- gray( (wordFreq+10) / (max(wordFreq)+10) )
wordcloud(words=names(wordFreq), freq=wordFreq, min.freq=50, random.order=F,
colors=pal)
source('~/NUS/TU Delft/WI4231 Mathematical Data Science/Exercises/Week 3/TweetCleaner.R', echo=TRUE)
m = as.matrix(tdm)
wordFreq <- sort(rowSums(m), decreasing=TRUE)
pal <- brewer.pal(9, "GnBu")
pal <- pal[-(1:4)]
set.seed(375) # to make it reproducible
grayLevels <- gray( (wordFreq+10) / (max(wordFreq)+10) )
wordcloud(words=names(wordFreq), freq=wordFreq, min.freq=50, random.order=F,
colors=pal)
(koRpus)
(koRpus)
library(koRpus)
library(qdapDictionaries)
library(qdap);library(stopwords);library(SnowballC)
library(tm)
library(ggplot2)
library(wordcloud)
m = as.matrix(tdm)
wordFreq <- sort(rowSums(m), decreasing=TRUE)
pal <- brewer.pal(9, "GnBu")
pal <- pal[-(1:4)]
set.seed(375) # to make it reproducible
grayLevels <- gray( (wordFreq+10) / (max(wordFreq)+10) )
wordcloud(words=names(wordFreq), freq=wordFreq, min.freq=50, random.order=F,
colors=pal)
library(koRpus)
library(qdapDictionaries)
library(qdap);library(stopwords);library(SnowballC)
library(tm)
library(ggplot2)
library(wordcloud)
View(dataframe)
tweets2=tweets
View(tweets2)
tweets2$text = dataframe
processed = textProcessor(tweets2$text, metadata = tweets2)
library(stm)
install.packages("stm")
processed = textProcessor(tweets2$text, metadata = tweets2)
library(stm)
processed = textProcessor(tweets2$text, metadata = tweets2)
processed = textProcessor(tweets2$text, metadata = tweets)
processed = textProcessor(tweets$text, metadata = tweets)
plotRemoved(processed$documents, lower.thresh = seq(1, 40, by = 1))
out <- prepDocuments(processed$documents,processed$vocab,processed$meta,
lower.thresh = 2)
docs <- out$documents
vocab <- out$vocab
meta <- out$meta
if(length(out$docs.removed)>0){
reviews <- reviews[-out$docs.removed,]
}
tweets2=tweets
tweets2=tweets
processed = textProcessor(tweets2$text, metadata = tweets2)
plotRemoved(processed$documents, lower.thresh = seq(1, 40, by = 1))
out <- prepDocuments(processed$documents,processed$vocab,processed$meta,
lower.thresh = 2)
docs <- out$documents
vocab <- out$vocab
meta <- out$meta
if(length(out$docs.removed)>0){
tweets2s <- tweets2[-out$docs.removed,]
}
storage <- searchK(out$documents, out$vocab, K = seq(5,20,by=1),
prevalence =~ location, data = meta)
storage <- searchK(out$documents, out$vocab, K = seq(5,20,by=1),
data = meta)
ggplot(data=storage$results, aes(semcoh,exclus,
label=5:20)) + geom_text(size=8) +
ylab("Exclusivity") + xlab("Semantic Coherence") +
theme(axis.title.x = element_text(vjust=-0.5),axis.text=element_text(size=16)) +
theme(axis.title.y = element_text(vjust=1.3),axis.title=element_text(size=16))
fit9 <- stm(out$documents, out$vocab, K = 9,
max.em.its = 120,	data = out$meta, init.type="Spectral")
fit12 <- stm(out$documents, out$vocab, K = 12,
max.em.its = 200,	data = out$meta, init.type="Spectral")
fit20 <- stm(out$documents, out$vocab, K = 20,
max.em.its = 200,	data = out$meta, init.type="Spectral")
labelTopics(fit9,n=9)
labelTopics(fit9,n=15)
shortdocs = tweets2$text
findThoughts(fit9,texts = shortdocs, n = 5,topics = 9)
findThoughts(fit9,texts = shortdocs, n = 9,topics = 9)
findThoughts(fit9,texts = shortdocs, n = NULL,topics = 9)
labelTopics(fit9,n=15)
findThoughts(fit9,texts = shortdocs, n = 10,topics = 9)
findThoughts(fit9,texts = shortdocs, n = 10,topics = 10)
if(length(out$docs.removed)>0){
tweets2s <- tweets2[-out$docs.removed,]
}
shortdocs = tweets2$text
findThoughts(fit9,texts = shortdocs, n = 10,topics = 9)
rm(tweets2s)
if(length(out$docs.removed)>0){
tweets2 <- tweets2[-out$docs.removed,]
}
shortdocs = tweets2$text
findThoughts(fit9,texts = shortdocs, n = 10,topics = 9)
labelTopics(fit9,n=15)
findThoughts(fit9,texts = shortdocs, n = 10,topics = 9)
labelTopics(fit9,n=15)
findThoughts(fit9,texts = shortdocs, n = 10,topics = 1)
TopicMeans <- matrix(0,9,1)
for(i in 1:9){
TopicMeans[i,] <- tapply(fit9$theta[,i],mean)
}
for(i in 1:9){
TopicMeans[i,] <- mean(fit9$theta[,i])
}
label <- c('Topic 1: Super Tuesday','Topic 2: Campaign',
'Topic 3: Badmouthing Others','Topic 4: Thanking Republican Supporters',
'Topic 5: FBI and Scandals','Topic 6: Trump Slogans and Rhetoric (MAGA)',
'Topic 7: Republicans in News/Interviews','Topic 8: Policies (Border wall, Jobs)',
'Topic 9: Trump & Megyn Kelly Feud')
TopicMeans <- matrix(0,9,1)
for(i in 1:9){
TopicMeans[i,] <- mean(fit9$theta[,i])
}
colnames(TopicMeans) <- 'Frequency'
rownames(TopicMeans) <- label
TopicMeans <- as.data.frame(TopicMeans)
RoundedTopicMeans=round(TopicMeans,2)
View(RoundedTopicMeans)
findThoughts(fit9,texts = shortdocs, n = 10,topics = 2)
label <- c('Topic 1: Super Tuesday','Topic 2: Campaign',
'Topic 3: Badmouthing Others (Rubio, Obama, Clinton, Democrats)','Topic 4: Thanking Republican Supporters',
'Topic 5: FBI and Scandals','Topic 6: Trump Slogans and Rhetoric (MAGA)',
'Topic 7: Republicans in News/Interviews','Topic 8: Policies (Border wall, Jobs)',
'Topic 9: Trump & Megyn Kelly Feud')
colnames(TopicMeans) <- 'Frequency'
rownames(TopicMeans) <- label
TopicMeans <- as.data.frame(TopicMeans)
RoundedTopicMeans=round(TopicMeans,2)
topicCorr(fit9)
plot(topicCorr(fit9))
findThoughts(fit9,texts = shortdocs, n = 10,topics = 9)
for(i in 1:9) {
cloud(poliblogPrevFit, topic = i, scale = c(2,.25))
}
cloud(fit9, topic = i, scale = c(2,.25))
for(i in 1:9) {
cloud(fit9, topic = i, scale = c(2,.25))
}
plot(topicCorr(fit9))
topicCorr(fit9)
plot(topicCorr(fit9))
plot(topicCorr(fit9))
plot(topicCorr(fit9))
for(i in 1:9) {
cloud(fit9, topic = i, scale = c(2,.25))
}
plot(topicCorr(fit9))
for(i in 1:9) {
cloud(fit9, topic = i, scale = c(4,.65))
}
par(mfrow=c(3,3))
for(i in 1:9) {
cloud(fit9, topic = i, scale = c(4,.65))
}
par(mfrow=c(3,3))
for(i in 1:9) {
cloud(fit9, topic = i, scale = c(4,.65), color=pal)
}
par(mfrow=c(3,3))
for(i in 1:9) {
cloud(fit9, topic = i, scale = c(4,.65), color='dodgerblue')
}
par(mfrow=c(3,3))
for(i in 1:9) {
cloud(fit9, topic = i, scale = c(4,.65), color='navy')
}
par(mfrow=c(3,3))
for(i in 1:9) {
cloud(fit9, topic = i, scale = c(4,.65), color='navy', max.words = 20)
}
cloud(fit9, topic = i, scale = c(4,.65), color='navy', max.words = 25)
par(mfrow=c(3,3))
for(i in 1:9) {
cloud(fit9, topic = i, scale = c(4,.65), color='navy', max.words = 25)
}
cloud(fit9, topic = i, scale = c(4,.65), color=pal, max.words = 25)
par(mfrow=c(3,3))
for(i in 1:9) {
cloud(fit9, topic = i, scale = c(4,.65), color=pal, max.words = 25)
}
par(mfrow=c(3,3))
for(i in 1:9) {
cloud(fit9, topic = i, scale = c(4,.65), color=pal, max.words = 30)
}
paste('meow', 2)
par(mfrow=c(3,3))
for(i in 1:9) {
cloud(fit9, topic = i, scale = c(4,.65), color=pal, max.words = 30, main = paste('Topic', i))
}
cloud(fit9, topic = i, scale = c(4,.65), color=pal, max.words = 30, main = paste('Topic', i))
warnings()
par(mfrow=c(3,3))
for(i in 1:9) {
cloud(fit9, topic = i, scale = c(4,.65), color=pal, max.words = 30)
}
par(mfrow=c(3,3))
for(i in 1:9) {
cloud(fit9, topic = i, scale = c(4,.65), color='navy', max.words = 30)
}
par(mfrow=c(3,3))
for(i in 1:9) {
cloud(fit9, topic = i, scale = c(4,.65), color='navy', max.words = 40)
}
par(mfrow=c(3,3))
for(i in 1:9) {
cloud(fit9, topic = i, scale = c(4,.65), color='navy', max.words = 40)
}
