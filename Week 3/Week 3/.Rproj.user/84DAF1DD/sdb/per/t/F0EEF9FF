{
    "collab_server" : "",
    "contents" : "library(koRpus)\nlibrary(qdapDictionaries)\nlibrary(qdap);library(stopwords);library(SnowballC)\nlibrary(tm)\nlibrary(ggplot2)\nlibrary(wordcloud)\nlibrary(stm)\n\ntweets=read.csv('tweets.csv',header=T, quote = \"\", row.names = NULL,stringsAsFactors = FALSE)\nattach(tweets)\n\nmyUneditedCorpus = Corpus(VectorSource(text))\n\n###TRANSFORMING TEXT\n# build a corpus, and specify the source to be character vectors\nmyCorpus <- Corpus(VectorSource(text))\n# convert to lower case # myCorpus <- tm_map(myCorpus, tolower)\n# tm v0.6\nmyCorpus <- tm_map(myCorpus, content_transformer(tolower))\n# replace U.S. with USA\nreplaceUS <- function(x) gsub(\"u.s.\", \" america \", x)\n# replace punctuation with space\nremovePunc <- function(x) gsub(\"[[:punct:]]\", \" \", x)\n# remove &amp\nremoveAndSymbol <- function(x) gsub(\"&amp\", \" \", x)\n# remove username\nremoveUsername <- function(x) gsub(\"@\\\\w+\", \" \", x)\n# remove URLs\nremoveURL <- function(x) gsub(\"http[^[:space:]]*\", \" \", x)\n# tm v0.6\nmyCorpus <- tm_map(myCorpus, content_transformer(removeAndSymbol))\nmyCorpus <- tm_map(myCorpus, content_transformer(removeUsername))\nmyCorpus <- tm_map(myCorpus, content_transformer(replaceUS))\nmyCorpus <- tm_map(myCorpus, content_transformer(removeURL))\n# myCorpus <- tm_map(myCorpus, content_transformer(removePunc))\n# remove anything other than English letters or space\nremoveNumPunct <- function(x) gsub(\"[^[:alpha:][:space:]]*\", \"\", x)\nmyCorpus <- tm_map(myCorpus, content_transformer(removeNumPunct))\n# add two extra stop words: \"available\" and \"via\"\nmyStopwords <- c(letters, \"available\", \"via\", \"rt\", stopwords(\"english\"))\n# remove \"r\" and \"big\" from stopwords\nmyStopwords <- setdiff(myStopwords, c(\"r\", \"big\"))\n# remove stopwords from corpus\nmyCorpus <- tm_map(myCorpus, removeWords, myStopwords)\n# remove extra whitespace\nmyCorpus <- tm_map(myCorpus, stripWhitespace)\n\n\n###STEMMING WORDS\n# keep a copy of corpus to use later as a dictionary for stem completion\nmyCorpusCopy <- myCorpus\n# stem words\nmyCorpus <- tm_map(myCorpus, stemDocument)\n# inspect documents (tweets) numbered 11 to 15\ninspect(myCorpus[11:15])\n\nstemCompletion_mod <- function(x,dict=dictCorpus) {\n  PlainTextDocument(stripWhitespace(paste(stemCompletion(gsub(\"hillari\", \"hillary\", unlist(strsplit(as.character(x),\" \"))),dictionary=dict, type=\"prevalent\"),sep=\"\", collapse=\" \")))\n}\n\nmyCorpus2 <- lapply(myCorpus, stemCompletion_mod, dict = myCorpusCopy)\ndataframe <- data.frame(text=unlist(sapply(myCorpus2, `[`, \"content\")), \n                        stringsAsFactors=F)\n\n\nmyCorpus3 = Corpus(VectorSource(dataframe$text))\n\ninspect(myCorpus3[11:15])\n\n\n\n### TERM DOC MAT\ntdm <- TermDocumentMatrix(myCorpus3, control=list(wordLengths=c(1,Inf)))\ntdm\nidx <- which(dimnames(tdm)$Terms == \"r\")\ninspect(tdm)\ntdmUnedited = TermDocumentMatrix(myUneditedCorpus, control=list(wordLengths=c(1,Inf)))\ntdmUnedited\n\n### TERM DOC MAT 2 (without word completion)\ntdm2  <- TermDocumentMatrix(myCorpus)\ntdm2\n\n###WORD FREQ unedited vs processed text\nfindFreqTerms(tdm,lowfreq=190)             #most freq words after processing\nfindFreqTerms(tdmUnedited,lowfreq=290)     #most freq words originally (take note stopwords are most freq)\ninspect(myCorpus3[20:24])                  #tweets after processing\ninspect(myCorpus[20:24])                   #tweets after stemming, before stem completion\ninspect(myUneditedCorpus[20:24])           #original tweets\n\n# Most frequent terms for stem completed words\ntermFreq = rowSums(as.matrix(tdm))\ntermFreq = subset(termFreq, termFreq>=190)\ndf <- data.frame(term=names(termFreq), freq=termFreq)\ndf <- transform(df, term = reorder(term, freq))\nggplot(df, aes(x=term, y=freq, fill=freq)) + \n  geom_bar(stat=\"identity\") + \n  xlab(\"Terms\") + ylab(\"Count\") + coord_flip()  +\n  geom_text(aes(x=term, y=freq,label=freq), colour=\"black\", fontface = 'bold', family = 'sans', size=4, hjust=-0.2) + \n  scale_fill_gradient(name = legend, high = 'dodgerblue', low = 'deepskyblue', guide = FALSE) + \n  theme_bw() +\n  theme(panel.border = element_blank(), panel.grid.major = element_blank(), panel.grid.minor = element_blank(), \n        axis.title.x=element_blank(), axis.ticks.x=element_blank(), \n        axis.title.y=element_blank(), axis.ticks.y=element_blank(),\n        plot.title = element_text(hjust = 0.5), axis.line = element_line(colour = \"black\")) +\n  ggtitle(\"Most Frequent Words\")\n\n# Most frequent terms for non stem completed words\ntermFreq2 = rowSums(as.matrix(tdm2))\ntermFreq2 = subset(termFreq2, termFreq2>=190)\ndf <- data.frame(term=names(termFreq2), freq=termFreq2)\ndf <- transform(df, term = reorder(term, freq))\nggplot(df, aes(x=term, y=freq, fill=freq)) + \n  geom_bar(stat=\"identity\") + \n  xlab(\"Terms\") + ylab(\"Count\") + coord_flip()  +\n  geom_text(aes(x=term, y=freq,label=freq), colour=\"black\", fontface = 'bold', family = 'sans', size=4, hjust=-0.2) + \n  scale_fill_gradient(name = legend, high = 'dodgerblue', low = 'deepskyblue', guide = FALSE) + \n  theme_bw() +\n  theme(panel.border = element_blank(), panel.grid.major = element_blank(), panel.grid.minor = element_blank(), \n        axis.title.x=element_blank(), axis.ticks.x=element_blank(), \n        axis.title.y=element_blank(), axis.ticks.y=element_blank(),\n        plot.title = element_text(hjust = 0.5), axis.line = element_line(colour = \"black\")) +\n  ggtitle(\"Most Frequent Words (not stem completed)\")\n\n###WORD CLOUD\nm = as.matrix(tdm)\n# calculate the frequency of words and sort it descendingly by frequency\nwordFreq <- sort(rowSums(m), decreasing=TRUE)\n# colors\npal <- brewer.pal(9, \"GnBu\")\npal <- pal[-(1:4)]\n# word cloud \nset.seed(375) # to make it reproducible\ngrayLevels <- gray( (wordFreq+10) / (max(wordFreq)+10) )\nwordcloud(words=names(wordFreq), freq=wordFreq, min.freq=50, random.order=F, \n          colors=pal)\n\n# remove sparse terms\ntdm2 <- removeSparseTerms(tdm, sparse=0.95)\nm2 <- as.matrix(tdm2)\n##k-means\n# transpose the matrix to cluster documents (tweets)\nm3 <- t(m2)\n# set a fixed random seed\nset.seed(122)\n\n#Elbow Method for finding the optimal number of clusters\n# Compute and plot wss for k = 2 to k = 15.\nk.max <- 15\ndata <- m2\nwss <- sapply(1:k.max, \n              function(k){kmeans(data, k, nstart=50,iter.max = 15 )$tot.withinss})\nprint(wss)\nplot(1:k.max, wss,\n     type=\"b\", pch = 19, frame = FALSE, \n     xlab=\"Number of clusters K\",\n     ylab=\"Total within-clusters sum of squares\")\n\n# k-means clustering of tweets\nk <- 6\nkmeansResult <- kmeans(m3, k)\n# cluster centers\nround(kmeansResult$centers, digits=3)\n\nfor (i in 1:k) {\n  cat(paste(\"cluster \", i, \":  \", sep=\"\"))\n  s <- sort(kmeansResult$centers[i,], decreasing=T)\n  cat(names(s)[1:3], \"\\n\")\n  \n  # print the tweets of every cluster\n  # print(tweet[which(kmeansResult$cluster==i)]) \n}\n\n##hierarchical clustering\ndistMatrix <- dist(scale(m2))\nfit <- hclust(distMatrix, method=\"ward.D\")\nplot(fit)\n# cut tree into 6 clusters\nrect.hclust(fit, k=6)\n(groups <- cutree(fit, k=6))\n\n\n\n###STM\ntweets2=tweets\nprocessed = textProcessor(tweets2$text, metadata = tweets2)\nplotRemoved(processed$documents, lower.thresh = seq(1, 40, by = 1))\nout <- prepDocuments(processed$documents,processed$vocab,processed$meta,\n                     lower.thresh = 2)\ndocs <- out$documents\nvocab <- out$vocab\nmeta <- out$meta\nif(length(out$docs.removed)>0){\n  tweets2 <- tweets2[-out$docs.removed,]\n}\nstorage <- searchK(out$documents, out$vocab, K = seq(5,20,by=1),\n                    data = meta)\n\nggplot(data=storage$results, aes(semcoh,exclus,\n                                 label=5:20)) + geom_text(size=8) +\n  ylab(\"Exclusivity\") + xlab(\"Semantic Coherence\") +\n  theme(axis.title.x = element_text(vjust=-0.5),axis.text=element_text(size=16)) + \n  theme(axis.title.y = element_text(vjust=1.3),axis.title=element_text(size=16))\n#9, 12, 20\n\nshortdocs = tweets2$text\n\nfit9 <- stm(out$documents, out$vocab, K = 9,\n            max.em.its = 120,\tdata = out$meta, init.type=\"Spectral\")\nlabelTopics(fit9,n=15)\nfindThoughts(fit9,texts = shortdocs, n = 10,topics = 9)\n\nlabel <- c('Topic 1: Super Tuesday','Topic 2: Campaign',\n           'Topic 3: Badmouthing Others (Rubio, Obama, Clinton, Democrats)','Topic 4: Thanking Republican Supporters',\n           'Topic 5: FBI and Scandals','Topic 6: Trump Slogans and Rhetoric (MAGA)',\n           'Topic 7: Republicans in News/Interviews','Topic 8: Policies (Border wall, Jobs)',\n           'Topic 9: Trump & Megyn Kelly Feud')\n\n## Topic Proportion\nTopicMeans <- matrix(0,9,1)\nfor(i in 1:9){\n  TopicMeans[i,] <- mean(fit9$theta[,i])\n}\ncolnames(TopicMeans) <- 'Frequency'\nrownames(TopicMeans) <- label\nTopicMeans <- as.data.frame(TopicMeans)\nRoundedTopicMeans=round(TopicMeans,2)\ntopicCorr(fit9)\nplot(topicCorr(fit9))\n\npar(mfrow=c(3,3))\nfor(i in 1:9) {\n  cloud(fit9, topic = i, scale = c(4,.65), color='navy', max.words = 40)\n}\n\n\n",
    "created" : 1520175846696.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "2884184322",
    "id" : "F0EEF9FF",
    "lastKnownWriteTime" : 1520953012,
    "last_content_update" : -2147483648,
    "path" : "~/Downloads/WI4231 Mathematical Data Science/Week 3/Week 3/TweetCleaner.R",
    "project_path" : "TweetCleaner.R",
    "properties" : {
        "tempName" : "Untitled1"
    },
    "relative_order" : 1,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}