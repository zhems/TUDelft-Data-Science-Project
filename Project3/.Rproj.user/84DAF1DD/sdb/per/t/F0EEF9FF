{
    "collab_server" : "",
    "contents" : "library(koRpus)\nlibrary(qdapDictionaries)\nlibrary(qdap);library(stopwords);library(SnowballC)\nlibrary(tm)\nlibrary(ggplot2)\nlibrary(wordcloud)\n\ntweets=read.csv('tweets.csv',header=T, quote = \"\", row.names = NULL,stringsAsFactors = FALSE)\nattach(tweets)\n\nmyUneditedCorpus = Corpus(VectorSource(text))\n\n###TRANSFORMING TEXT\n# build a corpus, and specify the source to be character vectors\nmyCorpus <- Corpus(VectorSource(text))\n# convert to lower case # myCorpus <- tm_map(myCorpus, tolower)\n# tm v0.6\nmyCorpus <- tm_map(myCorpus, content_transformer(tolower))\n# remove URLs\nremoveURL <- function(x) gsub(\"http[^[:space:]]*\", \"\", x)\n# tm v0.6\nmyCorpus <- tm_map(myCorpus, content_transformer(removeURL))\n# remove anything other than English letters or space\nremoveNumPunct <- function(x) gsub(\"[^[:alpha:][:space:]]*\", \"\", x)\nmyCorpus <- tm_map(myCorpus, content_transformer(removeNumPunct))\n# add two extra stop words: \"available\" and \"via\"\nmyStopwords <- c(stopwords(\"english\"), \"available\", \"via\")\n# remove \"r\" and \"big\" from stopwords\nmyStopwords <- setdiff(myStopwords, c(\"r\", \"big\"))\n# remove stopwords from corpus\nmyCorpus <- tm_map(myCorpus, removeWords, myStopwords)\n# remove extra whitespace\nmyCorpus <- tm_map(myCorpus, stripWhitespace)\n\n\n###STEMMING WORDS\n# keep a copy of corpus to use later as a dictionary for stem completion\nmyCorpusCopy <- myCorpus\n# stem words\nmyCorpus <- tm_map(myCorpus, stemDocument)\n# inspect documents (tweets) numbered 11 to 15\ninspect(myCorpus[11:15])\n\nstemCompletion_mod <- function(x,dict=dictCorpus) {\n  PlainTextDocument(stripWhitespace(paste(stemCompletion(unlist(strsplit(as.character(x),\" \")),dictionary=dict, type=\"shortest\"),sep=\"\", collapse=\" \")))\n}\n\nmyCorpus2 <- lapply(myCorpus, stemCompletion_mod, dict = myCorpusCopy)\n\ndataframe <- data.frame(text=unlist(sapply(myCorpus2, `[`, \"content\")), \n                        stringsAsFactors=F)\n\n# tweets2=tweets\n# tweets[[2]]=dataframe$text\n# \n# write.csv(tweets, file='tweetsprocessed.csv')\n# \n# tp = read.csv('tweetsprocessed.csv',header=T, quote = \"\", row.names = NULL,stringsAsFactors = FALSE)\n\nmyCorpus3 = Corpus(VectorSource(dataframe$text))\n\ninspect(myCorpus3[11:15])\n\n# # # tm v0.6\n# stemCompletion2 <- function(x, dictionary) {\n#   x <- unlist(strsplit(as.character(x), \" \"))\n#   # Unexpectedly, stemCompletion completes an empty string to\n#   # a word in dictionary. Remove empty string to avoid above issue.\n#   x <- x[x != \"\"]\n#   x <- stemCompletion(x, dictionary=dictionary)\n#   x <- paste(x, sep=\"\", collapse=\" \")\n#   PlainTextDocument(stripWhitespace(x))\n# }\n# myCorpus <- lapply(myCorpus, stemCompletion2, dictionary=myCorpusCopy)\n# \n# myCorpus2 <- Corpus(VectorSource(myCorpus))\n# \n# inspect(myCorpus2[11:15])\n# \n# myCorpus <- Corpus(VectorSource(myCorpus))\n# \n# inspect(myCorpus[11:15])\n\n\n### TERM DOC MAT\ntdm <- TermDocumentMatrix(myCorpus3, control=list(wordLengths=c(1,Inf)))\ntdm\nidx <- which(dimnames(tdm)$Terms == \"r\")\ninspect(tdm[idx+(0:5),101:110])\ntdmUnedited = TermDocumentMatrix(myUneditedCorpus, control=list(wordLengths=c(1,Inf)))\ntdmUnedited\n\n###WORD FREQ unedited vs processed text\nfindFreqTerms(tdm,lowfreq=190)             #most freq words after processing\nfindFreqTerms(tdmUnedited,lowfreq=290)     #most freq words originally (take note stopwords are most freq)\ninspect(myCorpus3[20:24])                  #tweets after processing\ninspect(myUneditedCorpus[20:24])           #original tweets\n\ntermFreq = rowSums(as.matrix(tdm))\ntermFreq = subset(termFreq, termFreq>=190)\ndf <- data.frame(term=names(termFreq), freq=termFreq)\nggplot(df, aes(x=term, y=freq)) + geom_bar(stat=\"identity\") + \n  xlab(\"Terms\") + ylab(\"Count\") + coord_flip()\n\n###WORD CLOUD\nm = as.matrix(tdm)\n# calculate the frequency of words and sort it descendingly by frequency\nwordFreq <- sort(rowSums(m), decreasing=TRUE)\n# colors\npal <- brewer.pal(9, \"BuGn\")\npal <- pal[-(1:4)]\n# word cloud\nset.seed(375) # to make it reproducible\ngrayLevels <- gray( (wordFreq+10) / (max(wordFreq)+10) )\nwordcloud(words=names(wordFreq), freq=wordFreq, min.freq=3, random.order=F, \n          colors=pal)\n",
    "created" : 1520175846696.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "2919584729",
    "id" : "F0EEF9FF",
    "lastKnownWriteTime" : 1520549311,
    "last_content_update" : -2147483648,
    "path" : "~/Downloads/WI4231 Mathematical Data Science/Project3/TweetCleaner.R",
    "project_path" : "TweetCleaner.R",
    "properties" : {
        "tempName" : "Untitled1"
    },
    "relative_order" : 1,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}