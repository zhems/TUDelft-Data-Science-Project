tweets=read.csv('tweets.csv',header=T, quote = "", row.names = NULL,stringsAsFactors = FALSE)
attach(tweets)
myUneditedCorpus = Corpus(VectorSource(text))
###TRANSFORMING TEXT
# build a corpus, and specify the source to be character vectors
myCorpus <- Corpus(VectorSource(text))
# convert to lower case # myCorpus <- tm_map(myCorpus, tolower)
# tm v0.6
myCorpus <- tm_map(myCorpus, content_transformer(tolower))
# remove URLs
removeURL <- function(x) gsub("http[^[:space:]]*", "", x)
# tm v0.6
myCorpus <- tm_map(myCorpus, content_transformer(removeURL))
# remove anything other than English letters or space
removeNumPunct <- function(x) gsub("[^[:alpha:][:space:]]*", "", x)
myCorpus <- tm_map(myCorpus, content_transformer(removeNumPunct))
# add two extra stop words: "available" and "via"
myStopwords <- c(stopwords("english"), "available", "via")
# remove "r" and "big" from stopwords
myStopwords <- setdiff(myStopwords, c("r", "big"))
# remove stopwords from corpus
myCorpus <- tm_map(myCorpus, removeWords, myStopwords)
# remove extra whitespace
myCorpus <- tm_map(myCorpus, stripWhitespace)
###STEMMING WORDS
# keep a copy of corpus to use later as a dictionary for stem completion
myCorpusCopy <- myCorpus
# stem words
myCorpus <- tm_map(myCorpus, stemDocument)
# inspect documents (tweets) numbered 11 to 15
inspect(myCorpus[11:15])
myCorpus <- lapply(myCorpus, stemCompletion, dictionary=myCorpusCopy)
myCorpus <- Corpus(VectorSource(myCorpus))
inspect(myCorpus[11:15])
tweets=read.csv('tweets.csv',header=T, quote = "", row.names = NULL,stringsAsFactors = FALSE)
attach(tweets)
myUneditedCorpus = Corpus(VectorSource(text))
###TRANSFORMING TEXT
# build a corpus, and specify the source to be character vectors
myCorpus <- Corpus(VectorSource(text))
# convert to lower case # myCorpus <- tm_map(myCorpus, tolower)
# tm v0.6
myCorpus <- tm_map(myCorpus, content_transformer(tolower))
# remove URLs
removeURL <- function(x) gsub("http[^[:space:]]*", "", x)
# tm v0.6
myCorpus <- tm_map(myCorpus, content_transformer(removeURL))
# remove anything other than English letters or space
removeNumPunct <- function(x) gsub("[^[:alpha:][:space:]]*", "", x)
myCorpus <- tm_map(myCorpus, content_transformer(removeNumPunct))
# add two extra stop words: "available" and "via"
myStopwords <- c(stopwords("english"), "available", "via")
# remove "r" and "big" from stopwords
myStopwords <- setdiff(myStopwords, c("r", "big"))
# remove stopwords from corpus
myCorpus <- tm_map(myCorpus, removeWords, myStopwords)
# remove extra whitespace
myCorpus <- tm_map(myCorpus, stripWhitespace)
###STEMMING WORDS
# keep a copy of corpus to use later as a dictionary for stem completion
myCorpusCopy <- myCorpus
# stem words
myCorpus <- tm_map(myCorpus, stemDocument)
# inspect documents (tweets) numbered 11 to 15
inspect(myCorpus[11:15])
myCorpus <- tm_map(myCorpus, content_transformer(stemCompletion), dictionary=myCorpusCopy)
inspect(myCorpus[11:15])
tweets=read.csv('tweets.csv',header=T, quote = "", row.names = NULL,stringsAsFactors = FALSE)
attach(tweets)
myUneditedCorpus = Corpus(VectorSource(text))
###TRANSFORMING TEXT
# build a corpus, and specify the source to be character vectors
myCorpus <- Corpus(VectorSource(text))
# convert to lower case # myCorpus <- tm_map(myCorpus, tolower)
# tm v0.6
myCorpus <- tm_map(myCorpus, content_transformer(tolower))
# remove URLs
removeURL <- function(x) gsub("http[^[:space:]]*", "", x)
# tm v0.6
myCorpus <- tm_map(myCorpus, content_transformer(removeURL))
# remove anything other than English letters or space
removeNumPunct <- function(x) gsub("[^[:alpha:][:space:]]*", "", x)
myCorpus <- tm_map(myCorpus, content_transformer(removeNumPunct))
# add two extra stop words: "available" and "via"
myStopwords <- c(stopwords("english"), "available", "via")
# remove "r" and "big" from stopwords
myStopwords <- setdiff(myStopwords, c("r", "big"))
# remove stopwords from corpus
myCorpus <- tm_map(myCorpus, removeWords, myStopwords)
# remove extra whitespace
myCorpus <- tm_map(myCorpus, stripWhitespace)
###STEMMING WORDS
# keep a copy of corpus to use later as a dictionary for stem completion
myCorpusCopy <- myCorpus
# stem words
myCorpus <- tm_map(myCorpus, stemDocument)
# inspect documents (tweets) numbered 11 to 15
inspect(myCorpus[11:15])
# # tm v0.6
stemCompletion2 <- function(x, dictionary) {
x <- unlist(strsplit(as.character(x), " "))
# Unexpectedly, stemCompletion completes an empty string to
# a word in dictionary. Remove empty string to avoid above issue.
x <- x[x != ""]
x <- stemCompletion(x, dictionary=dictionary)
x <- paste(x, sep="", collapse=" ")
PlainTextDocument(stripWhitespace(x))
}
myCorpus <- lapply(myCorpus, stemCompletion2, dictionary=myCorpusCopy)
myCorpus <- Corpus(VectorSource(myCorpus))
inspect(myCorpus[11:15])
library(koRpus)
library(qdapDictionaries)
library(qdap);library(stopwords);library(SnowballC)
library(tm)
inspect(myCorpus[11:15])
myCorpus$1`10`
myCorpus[1]$content
myCorpus[1]
get(string,myCorpus)
myCorpus[1]
myCorpus
get("content",myCorpus)
myCorpus$content
tweets=read.csv('tweets.csv',header=T, quote = "", row.names = NULL,stringsAsFactors = FALSE)
attach(tweets)
myUneditedCorpus = Corpus(VectorSource(text))
###TRANSFORMING TEXT
# build a corpus, and specify the source to be character vectors
myCorpus <- Corpus(VectorSource(text))
# convert to lower case # myCorpus <- tm_map(myCorpus, tolower)
# tm v0.6
myCorpus <- tm_map(myCorpus, content_transformer(tolower))
# remove URLs
removeURL <- function(x) gsub("http[^[:space:]]*", "", x)
# tm v0.6
myCorpus <- tm_map(myCorpus, content_transformer(removeURL))
# remove anything other than English letters or space
removeNumPunct <- function(x) gsub("[^[:alpha:][:space:]]*", "", x)
myCorpus <- tm_map(myCorpus, content_transformer(removeNumPunct))
# add two extra stop words: "available" and "via"
myStopwords <- c(stopwords("english"), "available", "via")
# remove "r" and "big" from stopwords
myStopwords <- setdiff(myStopwords, c("r", "big"))
# remove stopwords from corpus
myCorpus <- tm_map(myCorpus, removeWords, myStopwords)
# remove extra whitespace
myCorpus <- tm_map(myCorpus, stripWhitespace)
###STEMMING WORDS
# keep a copy of corpus to use later as a dictionary for stem completion
myCorpusCopy <- myCorpus
# stem words
myCorpus <- tm_map(myCorpus, stemDocument)
# inspect documents (tweets) numbered 11 to 15
inspect(myCorpus[11:15])
stemCompletion2 <- function(x, dictionary) {
x <- unlist(strsplit(as.character(x), " "))
# Unexpectedly, stemCompletion completes an empty string to
# a word in dictionary. Remove empty string to avoid above issue.
x <- x[x != ""]
x <- stemCompletion(x, dictionary=dictionary)
x <- paste(x, sep="", collapse=" ")
PlainTextDocument(stripWhitespace(x))
}
myCorpus <- lapply(myCorpus, stemCompletion2, dictionary=myCorpusCopy)
tweets=read.csv('tweets.csv',header=T, quote = "", row.names = NULL,stringsAsFactors = FALSE)
attach(tweets)
myUneditedCorpus = Corpus(VectorSource(text))
###TRANSFORMING TEXT
# build a corpus, and specify the source to be character vectors
myCorpus <- Corpus(VectorSource(text))
# convert to lower case # myCorpus <- tm_map(myCorpus, tolower)
# tm v0.6
myCorpus <- tm_map(myCorpus, content_transformer(tolower))
# remove URLs
removeURL <- function(x) gsub("http[^[:space:]]*", "", x)
# tm v0.6
myCorpus <- tm_map(myCorpus, content_transformer(removeURL))
# remove anything other than English letters or space
removeNumPunct <- function(x) gsub("[^[:alpha:][:space:]]*", "", x)
myCorpus <- tm_map(myCorpus, content_transformer(removeNumPunct))
# add two extra stop words: "available" and "via"
myStopwords <- c(stopwords("english"), "available", "via")
# remove "r" and "big" from stopwords
myStopwords <- setdiff(myStopwords, c("r", "big"))
# remove stopwords from corpus
myCorpus <- tm_map(myCorpus, removeWords, myStopwords)
# remove extra whitespace
myCorpus <- tm_map(myCorpus, stripWhitespace)
###STEMMING WORDS
# keep a copy of corpus to use later as a dictionary for stem completion
myCorpusCopy <- myCorpus
# stem words
myCorpus <- tm_map(myCorpus, stemDocument)
# inspect documents (tweets) numbered 11 to 15
inspect(myCorpus[11:15])
# # tm v0.6
stemCompletion2 <- function(x, dictionary) {
x <- unlist(strsplit(as.character(x), " "))
# Unexpectedly, stemCompletion completes an empty string to
# a word in dictionary. Remove empty string to avoid above issue.
x <- x[x != ""]
x <- stemCompletion(x, dictionary=dictionary)
x <- paste(x, sep="", collapse=" ")
PlainTextDocument(stripWhitespace(x))
}
myCorpus = tm_map(myCorpus, stemCompletion2, myCorpusCopy)
inspect(myCorpus[11:15])
tweets=read.csv('tweets.csv',header=T, quote = "", row.names = NULL,stringsAsFactors = FALSE)
attach(tweets)
myUneditedCorpus = Corpus(VectorSource(text))
###TRANSFORMING TEXT
# build a corpus, and specify the source to be character vectors
myCorpus <- Corpus(VectorSource(text))
# convert to lower case # myCorpus <- tm_map(myCorpus, tolower)
# tm v0.6
myCorpus <- tm_map(myCorpus, content_transformer(tolower))
# remove URLs
removeURL <- function(x) gsub("http[^[:space:]]*", "", x)
# tm v0.6
myCorpus <- tm_map(myCorpus, content_transformer(removeURL))
# remove anything other than English letters or space
removeNumPunct <- function(x) gsub("[^[:alpha:][:space:]]*", "", x)
myCorpus <- tm_map(myCorpus, content_transformer(removeNumPunct))
# add two extra stop words: "available" and "via"
myStopwords <- c(stopwords("english"), "available", "via")
# remove "r" and "big" from stopwords
myStopwords <- setdiff(myStopwords, c("r", "big"))
# remove stopwords from corpus
myCorpus <- tm_map(myCorpus, removeWords, myStopwords)
# remove extra whitespace
myCorpus <- tm_map(myCorpus, stripWhitespace)
###STEMMING WORDS
# keep a copy of corpus to use later as a dictionary for stem completion
myCorpusCopy <- myCorpus
# stem words
myCorpus <- tm_map(myCorpus, stemDocument)
# inspect documents (tweets) numbered 11 to 15
inspect(myCorpus[11:15])
# # tm v0.6
stemCompletion2 <- function(x, dictionary) {
x <- unlist(strsplit(as.character(x), " "))
# Unexpectedly, stemCompletion completes an empty string to
# a word in dictionary. Remove empty string to avoid above issue.
x <- x[x != ""]
x <- stemCompletion(x, dictionary=dictionary)
x <- paste(x, sep="", collapse=" ")
PlainTextDocument(stripWhitespace(x))
}
myCorpus <- lapply(myCorpus, stemCompletion2, dictionary=myCorpusCopy)
myCorpus2 <- Corpus(VectorSource(myCorpus$content))
inspect(myCorpus2[11:15])
myCorpus$content
myCorpus[0]$content
myCorpus[1]$content
myCorpus
myCorpus[[1]]
myCorpus[1]
get("content",myCorpus)
hello=get("content",myCorpus)
hello=get("content",eval(as.character(myCorpus)))
myCorpus2 <- Corpus(myCorpus)
myCorpus[1]
myCorpus[1]$content
myCorpus[1]$Content
myCorpus[1][1]
myCorpus[1][2]
myCorpus[1][1]
myCorpus[1][1][1]
myCorpus[2][1][1]
myCorpus[1]$x
inspect(myCorpus[11:15])
inspect(myCorpus[1])
inspect(myCorpus[[1]])
myCorpusCopy[1]
myCorpusCopy[[1]]
myCorpus[1]
myCorpus2 <- Corpus(VectorSource(myCorpus))
library(koRpus)
library(qdapDictionaries)
library(qdap);library(stopwords);library(SnowballC)
library(tm)
tweets=read.csv('tweets.csv',header=T, quote = "", row.names = NULL,stringsAsFactors = FALSE)
attach(tweets)
myUneditedCorpus = Corpus(VectorSource(text))
###TRANSFORMING TEXT
# build a corpus, and specify the source to be character vectors
myCorpus <- Corpus(VectorSource(text))
# convert to lower case # myCorpus <- tm_map(myCorpus, tolower)
# tm v0.6
myCorpus <- tm_map(myCorpus, content_transformer(tolower))
# remove URLs
removeURL <- function(x) gsub("http[^[:space:]]*", "", x)
# tm v0.6
myCorpus <- tm_map(myCorpus, content_transformer(removeURL))
# remove anything other than English letters or space
removeNumPunct <- function(x) gsub("[^[:alpha:][:space:]]*", "", x)
myCorpus <- tm_map(myCorpus, content_transformer(removeNumPunct))
# add two extra stop words: "available" and "via"
myStopwords <- c(stopwords("english"), "available", "via")
# remove "r" and "big" from stopwords
myStopwords <- setdiff(myStopwords, c("r", "big"))
# remove stopwords from corpus
myCorpus <- tm_map(myCorpus, removeWords, myStopwords)
# remove extra whitespace
myCorpus <- tm_map(myCorpus, stripWhitespace)
###STEMMING WORDS
# keep a copy of corpus to use later as a dictionary for stem completion
myCorpusCopy <- myCorpus
# stem words
myCorpus <- tm_map(myCorpus, stemDocument)
# inspect documents (tweets) numbered 11 to 15
inspect(myCorpus[11:15])
myCorpus2 <-tm_map(myCorpus, stemCompletion,dictionary=myCorpusCopy)
inspect(myCorpus2[11:15])
inspect(myCorpusCopy[11:15])
inspect(myCorpus[11:15])
myCorpus2 <-tm_map(myCorpus, stemCompletion,language = "english")
library(koRpus)
library(qdapDictionaries)
library(qdap);library(stopwords);library(SnowballC)
library(tm)
tweets=read.csv('tweets.csv',header=T, quote = "", row.names = NULL,stringsAsFactors = FALSE)
attach(tweets)
myUneditedCorpus = Corpus(VectorSource(text))
###TRANSFORMING TEXT
# build a corpus, and specify the source to be character vectors
myCorpus <- Corpus(VectorSource(text))
# convert to lower case # myCorpus <- tm_map(myCorpus, tolower)
# tm v0.6
myCorpus <- tm_map(myCorpus, content_transformer(tolower))
# remove URLs
removeURL <- function(x) gsub("http[^[:space:]]*", "", x)
# tm v0.6
myCorpus <- tm_map(myCorpus, content_transformer(removeURL))
# remove anything other than English letters or space
removeNumPunct <- function(x) gsub("[^[:alpha:][:space:]]*", "", x)
myCorpus <- tm_map(myCorpus, content_transformer(removeNumPunct))
# add two extra stop words: "available" and "via"
myStopwords <- c(stopwords("english"), "available", "via")
# remove "r" and "big" from stopwords
myStopwords <- setdiff(myStopwords, c("r", "big"))
# remove stopwords from corpus
myCorpus <- tm_map(myCorpus, removeWords, myStopwords)
# remove extra whitespace
myCorpus <- tm_map(myCorpus, stripWhitespace)
###STEMMING WORDS
# keep a copy of corpus to use later as a dictionary for stem completion
myCorpusCopy <- myCorpus
# stem words
myCorpus <- tm_map(myCorpus, stemDocument)
# inspect documents (tweets) numbered 11 to 15
inspect(myCorpus[11:15])
stemCompletion_mod <- function(x,dict=dictCorpus) {
PlainTextDocument(stripWhitespace(paste(stemCompletion(unlist(strsplit(as.character(x)," ")),dictionary=dict, type="shortest"),sep="", collapse=" ")))
}
myCorpus2 <- lapply(myCorpus, stemCompletion_mod, dict = myCorpusCopy)
myCorpus3 = Corpus(VectorSource(myCorpus2))
dataframe <- data.frame(text=unlist(sapply(myCorpus2, `[`, "content")),
stringsAsFactors=F)
View(dataframe)
myCorpus3 = Corpus(VectorSource(dataframe))
View(dataframe)
View(tweets)
tweets2=tweets
tweets2$text = dataframe
View(tweets2)
detach(tweets)
attach(tweets2)
myCorpus3 = Corpus(VectorSource(text))
myCorpus3 = Corpus(VectorSource(tweets$text))
detach(tweets2)
myCorpus3 = Corpus(VectorSource(tweets2$text.text))
tweets2=tweets
tweets2[1]
tweets2[2]
tweets2[[2]]
tweets2[[2]] = dataframe
tweets[1][1]
tweets[[1]][1]
tweets[[2]][1]
tweets2[[2]] = dataframe
tweets2=tweets
tweets2[[2]] = dataframe
View(tweets2)
myCorpus3 = Corpus(VectorSource(tweets2[[2]]))
myCorpus3 = Corpus(VectorSource(tweets2[2]))
dataframe <- data.frame(text=unlist(sapply(myCorpus2, `[`, "content")),
stringsAsFactors=T)
tweets2=tweets
tweets2[[2]] = dataframe
myCorpus3 = Corpus(VectorSource(tweets2[2]))
tweets2=tweets
tweets$text=tweets2[[2]]
View(tweets)
tweets$text=dataframe
myCorpus3 = Corpus(VectorSource(tweets$text))
write.csv(tweets, file='tweetsprocessed.csv')
tp = read.csv('tweets.csv',header=T, quote = "", row.names = NULL,stringsAsFactors = FALSE)
View(tp)
tp = read.csv('tweetsprocessed.csv',header=T, quote = "", row.names = NULL,stringsAsFactors = FALSE)
tweets=read.csv('tweets.csv',header=T, quote = "", row.names = NULL,stringsAsFactors = FALSE)
write.csv(tweets, file='tweetsprocessed.csv')
tweets[[2]]=dataframe
write.csv(tweets, file='tweetsprocessed.csv')
View(tweets)
attributes(dataframe)
View(dataframe)
dataframe
rownames(dataframe)=c()
dataframe
attributes(dataframe)
dataframe$names
dataframe$text
tweets=read.csv('tweets.csv',header=T, quote = "", row.names = NULL,stringsAsFactors = FALSE)
tweets2=tweets
tweets[[2]]=dataframe$text
write.csv(tweets, file='tweetsprocessed.csv')
tp = read.csv('tweetsprocessed.csv',header=T, quote = "", row.names = NULL,stringsAsFactors = FALSE)
myCorpus3 = Corpus(VectorSource(tweets$text))
tdm <- TermDocumentMatrix(myCorpus3, control=list(wordLengths=c(1,Inf)))
tdm
idx <- which(dimnames(tdm)$Terms == "r")
inspect(tdm[idx+(0:5),101:110])
inspect(myCorpus3[11:15])
tdm <- TermDocumentMatrix(myCorpus3, control=list(wordLengths=c(1,Inf)))
tdm
inspect(myCorpus[11:15])
tdm <- TermDocumentMatrix(myCorpus, control=list(wordLengths=c(1,Inf)))
tdm
inspect(myCorpus3[11:15])
tdm <- TermDocumentMatrix(myCorpus3, control=list(wordLengths=c(1,Inf)))
tdm
idx <- which(dimnames(tdm)$Terms == "r")
inspect(tdm[idx+(0:5),101:110])
myCorpus3 = Corpus(VectorSource(dataframe$text))
inspect(myCorpus3[11:15])
tdm <- TermDocumentMatrix(myCorpus3, control=list(wordLengths=c(1,Inf)))
tdm
idx <- which(dimnames(tdm)$Terms == "r")
inspect(tdm[idx+(0:5),101:110])
library(koRpus)
library(qdapDictionaries)
library(qdap);library(stopwords);library(SnowballC)
library(tm)
tdm
library(koRpus)
library(qdapDictionaries)
library(qdap);library(stopwords);library(SnowballC)
library(tm)
tdm
inspect(tdm[idx+(0:5),101:110])
inspect(tdm[idx+(0:5),101:510])
findFreqTerms(tdm,lowfreq=10)
findFreqTerms(tdm,lowfreq=100)
findFreqTerms(tdm,lowfreq=120)
findFreqTerms(tdm,lowfreq=180)
findFreqTerms(tdm,lowfreq=180)
findFreqTerms(tdm,lowfreq=150)
findFreqTerms(tdm,lowfreq=120)
findFreqTerms(tdm,lowfreq=190)
findFreqTerms(tdm,lowfreq=200)
findFreqTerms(tdm,lowfreq=270)
findFreqTerms(tdm,lowfreq=130)
findFreqTerms(tdm,lowfreq=125)
findFreqTerms(tdm,lowfreq=190)
tdm2 = TermDocumentMatrix(myUneditedCorpus, control=list(wordLengths=c(1,Inf)))
tdm2
findFreqTerms(tdm2,lowfreq=190)
findFreqTerms(tdm2,lowfreq=240)
findFreqTerms(tdm2,lowfreq=290)
tdm2 = TermDocumentMatrix(myCorpusCopy, control=list(wordLengths=c(1,Inf)))
tdm2
findFreqTerms(tdm2,lowfreq=290)
findFreqTerms(tdm2,lowfreq=190)
tdm[1]
tdm2 = TermDocumentMatrix(myUneditedCorpus, control=list(wordLengths=c(1,Inf)))
remove(tdm2)
tdmUnedited = TermDocumentMatrix(myUneditedCorpus, control=list(wordLengths=c(1,Inf)))
tdmUnedited
findFreqTerms(tdm,lowfreq=190)
findFreqTerms(tdmUnedited,lowfreq=190)
findFreqTerms(tdmUnedited,lowfreq=240)
findFreqTerms(tdmUnedited,lowfreq=270)
findFreqTerms(tdm,lowfreq=190)
findFreqTerms(tdmUnedited,lowfreq=270)
findFreqTerms(tdmUnedited,lowfreq=290)
findFreqTerms(tdm,lowfreq=190)
findFreqTerms(tdmUnedited,lowfreq=290)
myCorpus3[[1]]
myCorpus3[[1]]$text
inspect(myCorpus3[20:24])
inspect(myUneditedCorpus[20:24])
termFreq = rowSums(as.matrix(tdm))
termFreq = subset(termFreq, termFreq>=10)
termFreq = rowSums(as.matrix(tdm))
termFreq = subset(termFreq, termFreq>=190)
library(ggplot2)
df <- data.frame(term=names(termFreq), freq=termFreq)
ggplot(df, aes(x=term, y=freq)) + geom_bar(stat="identity") +
xlab("Terms") + ylab("Count") + coord_flip()
library(wordcloud)
m = as.matrix(tdm)
View(m)
m = as.matrix(tdm)
wordFreq <- sort(rowSums(m), decreasing=TRUE)
pal <- brewer.pal(9, "BuGn")
pal <- pal[-(1:4)]
set.seed(375) # to make it reproducible
grayLevels <- gray( (wordFreq+10) / (max(wordFreq)+10) )
wordcloud(words=names(wordFreq), freq=wordFreq, min.freq=3, random.order=F,
colors=pal)
